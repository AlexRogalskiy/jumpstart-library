{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Step-1 First verify the given keras model is working fine "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from local_utils import detect_lp\n",
    "from os.path import splitext,basename\n",
    "from tensorflow.keras.models  import model_from_json\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def save_model_h5_to_tf_format(path):\n",
    "    try:\n",
    "        path = splitext(path)[0]\n",
    "        #print(splitext(path.split('/')[0]))\n",
    "        with open('%s.json' % path, 'r') as json_file:\n",
    "            model_json = json_file.read()\n",
    "        model = model_from_json(model_json, custom_objects={})\n",
    "        model.load_weights('%s.h5' % path)\n",
    "        #print(os.path())\n",
    "        # Save the model to h5 format\n",
    "        model.save(\"models/wpod_net_all_in_one.h5\")\n",
    "        #print(\"Keras Model Saved successfully as h5 format\")\n",
    "        model = tf.keras.models.load_model('models/wpod_net_all_in_one.h5')\n",
    "        # Save the model to TF SavedModel format\n",
    "        tf.saved_model.save(model, \"models\")\n",
    "        #print(\"successfully saved keras model h5 file to tensorflow SavedModel format\")\n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def get_plate(image_path, Dmax=608, Dmin = 608):\n",
    "    vehicle = preprocess_image(image_path)\n",
    "    ratio = float(max(vehicle.shape[:2])) / min(vehicle.shape[:2])\n",
    "    side = int(ratio * Dmin)\n",
    "    bound_dim = min(side, Dmax)\n",
    "    _ , LpImg, _, cor = detect_lp(tf_model, vehicle, bound_dim, lp_threshold=0.5)\n",
    "    return vehicle, LpImg, cor\n",
    "\n",
    "def preprocess_image(image_path,resize=False):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img / 255\n",
    "    if resize:\n",
    "        img = cv2.resize(img, (224,224))\n",
    "    return img\n",
    "\n",
    "# Create sort_contours() function to grab the contour of each digit from left to right\n",
    "def sort_contours(cnts,reverse = False):\n",
    "    i = 0\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "                                        key=lambda b: b[1][i], reverse=reverse))\n",
    "    return cnts\n",
    "\n",
    "# pre-processing input images and pedict with model\n",
    "def predict_characters_from_model(image):\n",
    "    # Load model architecture, weight and labels\n",
    "    json_file = open('models/character_recoginition/MobileNets_character_recognition.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"models/character_recoginition/License_character_recognition_weight.h5\")\n",
    "    #print(\"[INFO] Model loaded successfully...\")\n",
    "    labels = LabelEncoder()\n",
    "    labels.classes_ = np.load('models/character_recoginition/license_character_classes.npy')\n",
    "    #print(\"[INFO] Labels loaded successfully...\")\n",
    "\n",
    "    image = cv2.resize(image,(80,80))\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "    prediction = labels.inverse_transform([np.argmax(model.predict(image[np.newaxis,:]))])\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # wpod_net_path = \"models/wpod-net.json\"\n",
    "# save_model_h5_to_tf_format(wpod_net_path)\n",
    "\n",
    "# # Just provide the directory name where the TF *.pb (model file saved_model.pb) file is located\n",
    "# tf_model = keras.models.load_model('models')\n",
    "# #print(tf_model.outputs)\n",
    "# #print(tf_model.inputs)\n",
    "\n",
    "# #print(\"Loading vehicle image for License Plate Detection...\")\n",
    "# input_image_path = \"dataset/plate3.jpeg\"\n",
    "\n",
    "# #print(\"Detecting License Plate...\")\n",
    "# vehicle, LpImg, cor = get_plate(input_image_path)\n",
    "\n",
    "# #fig = plt.figure(figsize=(12,6))\n",
    "# #grid = gridspec.GridSpec(ncols=2,nrows=1,figure=fig)\n",
    "\n",
    "# plate_image = cv2.convertScaleAbs(LpImg[0], alpha=(255.0))\n",
    "# gray = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)\n",
    "# blur = cv2.GaussianBlur(gray,(7,7),0)\n",
    "#  # Applied inversed thresh_binary \n",
    "# binary = cv2.threshold(blur, 180, 255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "# kernel3 = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "# thre_mor = cv2.morphologyEx(binary, cv2.MORPH_DILATE, kernel3)\n",
    "\n",
    "# cont, _  = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# # creat a copy version \"test_roi\" of plat_image to draw bounding box\n",
    "# test_roi = plate_image.copy()\n",
    "\n",
    "# # Initialize a list which will be used to append charater image\n",
    "# crop_characters = []\n",
    "\n",
    "# # define standard width and height of character\n",
    "# digit_w, digit_h = 30, 60\n",
    "\n",
    "# for c in sort_contours(cont):\n",
    "#     (x, y, w, h) = cv2.boundingRect(c)\n",
    "#     ratio = h/w\n",
    "#     if 1<=ratio<=3.5: # Only select contour with defined ratio\n",
    "#         if h/plate_image.shape[0]>=0.5: # Select contour which has the height larger than 50% of the plate\n",
    "#             # Draw bounding box arroung digit number\n",
    "#             cv2.rectangle(test_roi, (x, y), (x + w, y + h), (0, 255,0), 2)\n",
    "\n",
    "#             # Sperate number and gibe prediction\n",
    "#             curr_num = thre_mor[y:y+h,x:x+w]\n",
    "#             curr_num = cv2.resize(curr_num, dsize=(digit_w, digit_h))\n",
    "#             _, curr_num = cv2.threshold(curr_num, 220, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "#             crop_characters.append(curr_num)\n",
    "\n",
    "# #fig = plt.figure(figsize=(14,4))\n",
    "# #grid = gridspec.GridSpec(ncols=len(crop_characters),nrows=1,figure=fig)\n",
    "\n",
    "# # Load model architecture, weight and labels\n",
    "# json_file = open('models/character_recoginition/MobileNets_character_recognition.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# model = model_from_json(loaded_model_json)\n",
    "# model.load_weights(\"models/character_recoginition/License_character_recognition_weight.h5\")\n",
    "# #print(\"[INFO] Model loaded successfully...\")\n",
    "\n",
    "# labels = LabelEncoder()\n",
    "# labels.classes_ = np.load('models/character_recoginition/license_character_classes.npy')\n",
    "# #print(\"[INFO] Labels loaded successfully...\")\n",
    "\n",
    "\n",
    "# #fig = plt.figure(figsize=(15,3))\n",
    "# cols = len(crop_characters)\n",
    "# #grid = gridspec.GridSpec(ncols=cols,nrows=1,figure=fig)\n",
    "\n",
    "# final_string =  \" \"\n",
    "# for i,character in enumerate(crop_characters):\n",
    "#     title = np.array2string(predict_from_model(character,model,labels))\n",
    "#     final_string+=title.strip(\"'[]\")\n",
    "\n",
    "# print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_1_layer_call_fn, activation_1_layer_call_fn, conv2d_2_layer_call_fn, activation_2_layer_call_fn, conv2d_3_layer_call_fn while saving (showing 5 of 120). These functions will not be directly callable after loading.\n",
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "{\"license_plate_number_detection_status\": \"Successful\", \"detected_license_plate_number\": \"CH00SE\", \"input_image_name\": \"dataset/Cars53.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Successful\", \"detected_license_plate_number\": \"MH15BD8877\", \"input_image_name\": \"dataset/Cars52.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars51.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Successful\", \"detected_license_plate_number\": \"M0T\", \"input_image_name\": \"dataset/Cars45.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars41.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars54.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars42.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars57.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Successful\", \"detected_license_plate_number\": \"WMPW1\", \"input_image_name\": \"dataset/Cars59.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars58.png\"}\n",
      "{\"license_plate_number_detection_status\": \"Failed\", \"reason\": \"Not able to read license plate, it could be blur or complex image\", \"input_image_name\": \"dataset/Cars49.png\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def  lpr_process(input_image_path):\n",
    "    vehicle, LpImg, cor = get_plate(input_image_path)\n",
    "    plate_image = cv2.convertScaleAbs(LpImg[0], alpha=(255.0))\n",
    "    gray = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(7,7),0)\n",
    "     # Applied inversed thresh_binary \n",
    "    binary = cv2.threshold(blur, 180, 255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernel3 = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    thre_mor = cv2.morphologyEx(binary, cv2.MORPH_DILATE, kernel3)\n",
    "    \n",
    "    cont, _  = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # creat a copy version \"test_roi\" of plat_image to draw bounding box\n",
    "    test_roi = plate_image.copy()\n",
    "    # Initialize a list which will be used to append charater image\n",
    "    crop_characters = []\n",
    "    \n",
    "    # define standard width and height of character\n",
    "    digit_w, digit_h = 30, 60\n",
    "    \n",
    "    for c in sort_contours(cont):\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        ratio = h/w\n",
    "        if 1<=ratio<=3.5: # Only select contour with defined ratio\n",
    "            if h/plate_image.shape[0]>=0.5: # Select contour which has the height larger than 50% of the plate\n",
    "                # Draw bounding box arroung digit number\n",
    "                cv2.rectangle(test_roi, (x, y), (x + w, y + h), (0, 255,0), 2)\n",
    "                # Sperate number and gibe prediction\n",
    "                curr_num = thre_mor[y:y+h,x:x+w]\n",
    "                curr_num = cv2.resize(curr_num, dsize=(digit_w, digit_h))\n",
    "                _, curr_num = cv2.threshold(curr_num, 220, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                crop_characters.append(curr_num)\n",
    "    \n",
    "    cols = len(crop_characters)\n",
    "    \n",
    "    license_plate_string =  \"\"\n",
    "    for i,character in enumerate(crop_characters):\n",
    "        title = np.array2string(predict_characters_from_model(character))\n",
    "        license_plate_string+=title.strip(\"'[]\")\n",
    "    \n",
    "    if len(license_plate_string) >= 3 :\n",
    "        result = {\n",
    "            \"license_plate_number_detection_status\": \"Successful\",\n",
    "            \"detected_license_plate_number\": license_plate_string,\n",
    "            \"input_image_name\": input_image_path\n",
    "        }\n",
    "        print(json.dumps(result))\n",
    "    else:\n",
    "        result = {\n",
    "            \"license_plate_number_detection_status\": \"Failed\",\n",
    "            \"reason\": \"Not able to read license plate, it could be blur or complex image\",\n",
    "            \"input_image_name\": input_image_path\n",
    "        }\n",
    "        print(json.dumps(result))\n",
    "\n",
    "def main():\n",
    "    wpod_net_path = \"models/wpod-net.json\"\n",
    "    save_model_h5_to_tf_format(wpod_net_path)\n",
    "    # Just provide the directory name where the TF *.pb (model file saved_model.pb) file is located\n",
    "    \n",
    "\n",
    "   # input_image_path = \"dataset/plate5.jpeg\"\n",
    "    \n",
    "    # for image_id in range(0,11):\n",
    "    #     input_image_path = \"dataset/Cars\" + str(image_id) + \".png\"\n",
    "    #     license_plate_string = lpr_process(input_image_path)\n",
    "\n",
    "    dataset_directory = r'dataset'\n",
    "    for entry in os.scandir(dataset_directory):\n",
    "        if (entry.path.endswith(\".jpg\")\n",
    "               or entry.path.endswith(\".png\") or entry.path.endswith(\".jpeg\")) and entry.is_file():\n",
    "           license_plate_string = lpr_process(entry.path)\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    tf_model = keras.models.load_model('models')\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset/Cars53.png\ndataset/Cars52.png\ndataset/Cars51.png\ndataset/Cars45.png\ndataset/Cars41.png\ndataset/Cars54.png\ndataset/Cars42.png\ndataset/Cars57.png\ndataset/Cars59.png\ndataset/Cars58.png\ndataset/Cars49.png\n"
     ]
    }
   ],
   "source": [
    "# for image_name in range(0,11):\n",
    "#     path = \"dataset/Cars\" + str(image_name) + \".png\"\n",
    "#     print(path)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "directory = r'dataset'\n",
    "for entry in os.scandir(directory):\n",
    "    if (entry.path.endswith(\".jpg\")\n",
    "            or entry.path.endswith(\".png\") or entry.path.endswith(\".jpeg\")) and entry.is_file():\n",
    "        print(entry.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}