= Smart Cities Workshop Deployment Guide

== Requirements

* A working OpenShift environment. Last tested version: 4.7
* Cluster-admin access to OpenShift, or ability to deploy operators.
* OpenShift Data Foundation (ODF). Last tested version: 4.7
* A running RGW instance in ODF. External Route to the RGW must be exposed with SSL enabled for Grafana dashboards to work properly.

== Namespace

Create an OpenShift project/namespace to deploy the environment. In this documentation we'll use `smartcity`.

[source,bash]
----
oc new-project smartcity
----

TIP: If you did not use `smartcity` as the name of your project, don't forget to change it in the commands or the config files used for the deployment.

== Operators

If not already available, deploy the following operators from OperatorHub:

* Red Hat AMQ Streams (all namespaces)
* Grafana Operator (`smartcity` namespace only)
* Presto operator (`smartcity namespace only`)

== Deployment

From the `deploy` folder and subfolders, create the OpenShift resources in this order.

.Creating a resource
[source,bash]
----
oc apply -f file.yaml
----

=== Base elements

We will need a few base elements or helpers to start with. You can edit the Secret or ConfigMap files if you want change the default values. They contain the environment values that will be used by the rest of the deployments.

* `base/secret_postgresql.yaml`: Secrets to deploy the PostgreSQL database
* `base/dc_postgresql.yaml`: Deployment of the PostgreSQL helper database

=== Kafka

We will need two different Kafka instances. One will simulate the "Edges", the toll station, the other one the "Core". We will also create the different topics that are needed, as well as the Kafka Mirror Maker to replicate the topics from the Edge to the Core.

* `kafka/edge.yaml`: Edge Kafka instance
* `kafka/core.yaml`: Core Kafka instance
* `kafka/edge-topic.yaml`: Edge topic
* `kafka/core-topic.yaml`: Core topic
* `kafka/mirror.yaml`: Mirror maker
* `kafka/kafdrop.yaml`: Optional! Kafdrop is a UI interface to your Kafka cluster (to inspect messages)

=== Load Generator

This is the component that injects car images into the pipeline.

* `generator/obc_dataset_generator.yaml`: Bucket to store the images dataset
* `generator/is_generator.yaml`: ImageStream for the load generator
* `generator/bc_generator.yaml`: BuildConfiguration to create the load generator image
* `generator/dc_generator.yaml`: Deployment Configuration for the load generator


=== LPR Service

This component presents an API that you can query with an image and returns the infered licence plate number.

* `lpr_service/is_lpr_service.yaml`: ImageStream for the LPR service
* `lpr_service/bc_lpr_service.yaml`: BuildConfiguration for the LPR service
* `lpr_service/dc_lpr_service.yaml`: Deployment Configuration for the LPR service


=== Events Service

This is the component that runs in the Core and listens to incoming Kafka events to write them into a PostgreSQL database so that they can be queried to create the dashboards.

* `events_service/is_events_service.yaml`: ImageStream for the event service
* `events_service/bc_events_service.yaml`: BuildConfiguration for the event service
* `events_service/dc_events_service.yaml`: Deployment Configuration for the event service

=== Dataset

Retrieve the information for the dataset bucket created previously and upload the images.

[source,bash]
----
export AWS_ACCESS_KEY_ID=$(oc get secret/generator-dataset -o yaml | grep " AWS_ACCESS_KEY_ID" | awk '{ print $2 }' - | base64 -d)
export AWS_SECRET_ACCESS_KEY=$(oc get secret/generator-dataset -o yaml | grep " AWS_SECRET_ACCESS_KEY" | awk '{ print $2 }' - | base64 -d)
export RGW_ROUTE=https://$(oc get routes -n openshift-storage | grep rgw | awk '{ print $2 }')
export BUCKET=$(oc get cm/generator-dataset -o yaml | grep " BUCKET_NAME:" | awk '{ print $2 }' -)
aws --endpoint-url $RGW_ROUTE s3 cp --recursive ../source/dataset/images s3://$BUCKET/images
----


